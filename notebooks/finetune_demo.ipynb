{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5ff43",
   "metadata": {},
   "source": [
    "# Notebook: Fine-tune BERT on Your Labeled CSV Data\n",
    "\n",
    "### Objective:\n",
    "\n",
    "1. Walk through the entire fine-tuning pipeline step by step\n",
    "2. Explain what's going on in accessible language\n",
    "3. Prints out shapes, label distrubtions, and smaple rows at each stage so you can \"see\" the data evolving.\n",
    "4. Uses the `local_llm`'s core functions, so the notebook stays relatively clean and focused on _workflow_, not implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2c819",
   "metadata": {},
   "source": [
    "### Step 1 - Setup and Imports \n",
    "\n",
    "In this fist step, we import all the tools we need:\n",
    "- `pandas` to load and inspect the CSV.\n",
    "- The fine-tuning helpers form `local_llm.training.text_finetune`.\n",
    "\n",
    "We'll also define the paths to your data and BERT assets later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81108fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from local_llm.training.text_finetune import (\n",
    "    FineTuneConfig,\n",
    "    set_seed,\n",
    "    prepare_label_mapping,\n",
    "    stratified_split_indices,\n",
    "    encode_splits,\n",
    "    build_dataloaders,\n",
    "    build_bert_text_classifier_from_assets,\n",
    "    train_text_classifier,\n",
    "    evaluate_on_split,\n",
    "    save_finetuned_classifier,\n",
    "    export_predictions_csv,\n",
    ")\n",
    "\n",
    "print(\"Imports complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3daa01",
   "metadata": {},
   "source": [
    "### Step 2 - Load Your Labeled Data\n",
    "\n",
    "Now we: \n",
    "1. Load the labeled CSV file into a pandas DataFrame.\n",
    "2. Print:\n",
    "    - The first few rows\n",
    "    - Shape of the data (rows, columns)\n",
    "    - The Column Names\n",
    " \n",
    "This helps confirm that:\n",
    "- The file path is correct.\n",
    "- The column names match what we expect later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d1d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\Cameron.Webster\\Python\\local-llm\\data\\wbs_training_data.csv\n",
      "\n",
      "✅ Data loaded.\n",
      "Data shape: 19857 rows × 5 columns\n",
      "\n",
      "Columns:\n",
      "['parsid', 'wbs', 'wbs_name', 'keyword', 'level_1']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "parsid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wbs",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "wbs_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "level_1",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a6501e96-f5dc-4556-ad44-a62df6bca8ce",
       "rows": [
        [
         "0",
         "389",
         "3.1",
         "M&O Support",
         "m o support",
         "OPC Testing and Startup"
        ],
        [
         "1",
         "389",
         "3.2",
         "DOE Support",
         "doe support",
         "OPC Testing and Startup"
        ],
        [
         "2",
         "389",
         "2.1.0",
         "Enhanced Conceptual Design (ECD)",
         "conceptual design",
         "Design and NRE"
        ],
        [
         "3",
         "389",
         "2.1.1",
         "Preliminary Design (PD)",
         "preliminary design",
         "Design and NRE"
        ],
        [
         "4",
         "389",
         "2.1.2",
         "Final Design (FD)",
         "final design",
         "Design and NRE"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsid</th>\n",
       "      <th>wbs</th>\n",
       "      <th>wbs_name</th>\n",
       "      <th>keyword</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389</td>\n",
       "      <td>3.1</td>\n",
       "      <td>M&amp;O Support</td>\n",
       "      <td>m o support</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>3.2</td>\n",
       "      <td>DOE Support</td>\n",
       "      <td>doe support</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.0</td>\n",
       "      <td>Enhanced Conceptual Design (ECD)</td>\n",
       "      <td>conceptual design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.1</td>\n",
       "      <td>Preliminary Design (PD)</td>\n",
       "      <td>preliminary design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>389</td>\n",
       "      <td>2.1.2</td>\n",
       "      <td>Final Design (FD)</td>\n",
       "      <td>final design</td>\n",
       "      <td>Design and NRE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsid    wbs                          wbs_name             keyword  \\\n",
       "0     389    3.1                       M&O Support         m o support   \n",
       "1     389    3.2                       DOE Support         doe support   \n",
       "2     389  2.1.0  Enhanced Conceptual Design (ECD)   conceptual design   \n",
       "3     389  2.1.1           Preliminary Design (PD)  preliminary design   \n",
       "4     389  2.1.2                 Final Design (FD)        final design   \n",
       "\n",
       "                   level_1  \n",
       "0  OPC Testing and Startup  \n",
       "1  OPC Testing and Startup  \n",
       "2           Design and NRE  \n",
       "3           Design and NRE  \n",
       "4           Design and NRE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = Path(\"C:/Users/Cameron.Webster/Python/local-llm/data/wbs_training_data.csv\")  # adjust if your file lives elsewhere\n",
    "\n",
    "print(f\"Loading data from: {data_path.resolve()}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"\\n✅ Data loaded.\")\n",
    "print(f\"Data shape: {df.shape[0]} rows × {df.shape[1]} columns\\n\")\n",
    "print(\"Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fed667",
   "metadata": {},
   "source": [
    "### Step 3 - Define Fune-tuning Configuration\n",
    "\n",
    "Here we deine a `FineTuneConfig`, which is a single object that:\n",
    "- Tells the library which text columns touse and which columns is the label. \n",
    "- Controls how we split into train/validation/test sets.\n",
    "- Point to your BERT assets directory. \n",
    "- Sets training hyperparameters like learning rate and number of epochs.\n",
    "- Defines how much BERT we fine-tune(`finetune_policy` and `finetune_last_n`).\n",
    "\n",
    "We also call `set_seed` to make the run reproducible (same random splits each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51397d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuneConfig created:\n",
      "FineTuneConfig(text_cols=('wbs_name', 'keyword'), label_col='level_1', train_frac=0.8, val_frac=0.1, seed=42, assets_dir=WindowsPath('C:/Users/Cameron.Webster/Python/local-llm/assets/bert-large-local'), max_len=256, lowercase=True, batch_size=32, epochs=8, base_lr=2e-05, weight_decay=0.01, grad_clip_norm=1.0, finetune_policy='last_n', finetune_last_n=2, train_embeddings=False, pooling='cls', head_config=ClassifierHeadConfig(hidden_sizes=(768,), dropouts=(0.15, 0.2), use_layer_norm=True, activation='gelu'), device='cuda', output_dir=WindowsPath('C:/Users/Cameron.Webster/Python/local-llm/artifacts/finetune_large_bert'), run_name='finetune_run')\n",
      "\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "cfg = FineTuneConfig(\n",
    "    # Which text columns to concatenate into a single input string\n",
    "    text_cols=(\"wbs_name\", \"keyword\"),\n",
    "\n",
    "    # Which column contains the labels\n",
    "    label_col=\"level_1\",\n",
    "\n",
    "    # Where the converted BERT assets live\n",
    "    assets_dir=Path(\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-large-local\"),\n",
    "\n",
    "    # Where to save fine-tuning outputs (models, predictions, etc.)\n",
    "    output_dir=Path(\"C:/Users/Cameron.Webster/Python/local-llm/artifacts/finetune_large_bert\"),\n",
    "\n",
    "    # Train/val/test fractions (they should sum to <= 1, remainder is test)\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.10,\n",
    "\n",
    "    # Training loop hyperparameters\n",
    "    epochs=8,\n",
    "    base_lr=2e-5, \n",
    "    batch_size=32,\n",
    "\n",
    "    # How much of BERT to fine-tune\n",
    "    finetune_policy=\"last_n\",  # options: \"none\", \"last_n\", \"full\"\n",
    "    finetune_last_n=2,         # last 2 transformer layers trainable\n",
    "    train_embeddings=False,    # embeddings stay frozen\n",
    ")\n",
    "\n",
    "print(\"FineTuneConfig created:\")\n",
    "print(cfg)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(cfg.seed)\n",
    "print(f\"\\nRandom seed set to: {cfg.seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b658a9",
   "metadata": {},
   "source": [
    "### Step 4 - Map String Labels to Integer IDs\n",
    "\n",
    "Neural networks operate on numbers, not strings. \n",
    "\n",
    "Here we:\n",
    "- Convert the label columns (e.g., `\"Construction\"`, `\"Design\"`) into integers IDs.\n",
    "- Build two dictionaries:\n",
    "    - `label_to_id`: string label --> integer\n",
    "    - `id_to_label`: integer --> string label\n",
    "- Add a new column `label_id` to the DataFrame.\n",
    "\n",
    "We also print:\n",
    "- The mappings\n",
    "- How many examples there are per class (class balanace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bc95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing label mapping using label column: 'level_1'\n",
      "\n",
      "✅ Label mapping created.\n",
      "label_to_id:\n",
      "  'Construction' -> 0\n",
      "  'D&D' -> 1\n",
      "  'Design and NRE' -> 2\n",
      "  'OPC Testing and Startup' -> 3\n",
      "  'Process Equipment' -> 4\n",
      "  'SEPM' -> 5\n",
      "  'Site Preparation' -> 6\n",
      "  'Standard Equipment' -> 7\n",
      "\n",
      "id_to_label:\n",
      "  0 -> 'Construction'\n",
      "  1 -> 'D&D'\n",
      "  2 -> 'Design and NRE'\n",
      "  3 -> 'OPC Testing and Startup'\n",
      "  4 -> 'Process Equipment'\n",
      "  5 -> 'SEPM'\n",
      "  6 -> 'Site Preparation'\n",
      "  7 -> 'Standard Equipment'\n",
      "\n",
      "Value counts for label_id (class distribution):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9b5133af-3ca5-45d6-95cd-9a8ccbc1d0ac",
       "rows": [
        [
         "0",
         "2676"
        ],
        [
         "1",
         "509"
        ],
        [
         "2",
         "4112"
        ],
        [
         "3",
         "1551"
        ],
        [
         "4",
         "5198"
        ],
        [
         "5",
         "2880"
        ],
        [
         "6",
         "659"
        ],
        [
         "7",
         "2272"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    2676\n",
       "1     509\n",
       "2    4112\n",
       "3    1551\n",
       "4    5198\n",
       "5    2880\n",
       "6     659\n",
       "7    2272\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview with label_id:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level_1",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7c8efb1e-ee68-40e3-ba59-e2f507b50a7d",
       "rows": [
        [
         "0",
         "OPC Testing and Startup",
         "3"
        ],
        [
         "1",
         "OPC Testing and Startup",
         "3"
        ],
        [
         "2",
         "Design and NRE",
         "2"
        ],
        [
         "3",
         "Design and NRE",
         "2"
        ],
        [
         "4",
         "Design and NRE",
         "2"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_1</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   level_1  label_id\n",
       "0  OPC Testing and Startup         3\n",
       "1  OPC Testing and Startup         3\n",
       "2           Design and NRE         2\n",
       "3           Design and NRE         2\n",
       "4           Design and NRE         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_col = cfg.label_col\n",
    "print(f\"Preparing label mapping using label column: '{label_col}'\")\n",
    "\n",
    "df_mapped, label_to_id, id_to_label = prepare_label_mapping(df, label_col)\n",
    "\n",
    "print(\"\\n✅ Label mapping created.\")\n",
    "print(\"label_to_id:\")\n",
    "for lab, idx in label_to_id.items():\n",
    "    print(f\"  {lab!r} -> {idx}\")\n",
    "\n",
    "print(\"\\nid_to_label:\")\n",
    "for idx, lab in id_to_label.items():\n",
    "    print(f\"  {idx} -> {lab!r}\")\n",
    "\n",
    "print(\"\\nValue counts for label_id (class distribution):\")\n",
    "display(df_mapped[\"label_id\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nPreview with label_id:\")\n",
    "display(df_mapped[[label_col, \"label_id\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffc23c",
   "metadata": {},
   "source": [
    "### Step 5 - Stratified Train / Validation / Test Split\n",
    "\n",
    "We now split the data into three sets:\n",
    "- **Training**: used to fit the model. \n",
    "- **Validation**: used to tune hyperparameters and monitor overfitting.\n",
    "- **Test**: held out until the very end for final evaluation. \n",
    "\n",
    "The split is stratified, meaning:\n",
    "- Each set keeps apprximately the same label distribution as the full dataset. \n",
    "\n",
    "We print:\n",
    "- The sizes of each split.\n",
    "- Label distributions in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082f3b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stratified indices created.\n",
      "Train size: 15886\n",
      "Val size:   1986\n",
      "Test size:  1985\n",
      "\n",
      "Label distribution in TRAIN:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1e7540a9-778d-405a-aa8d-415070febb7a",
       "rows": [
        [
         "0",
         "2141"
        ],
        [
         "1",
         "407"
        ],
        [
         "2",
         "3290"
        ],
        [
         "3",
         "1241"
        ],
        [
         "4",
         "4158"
        ],
        [
         "5",
         "2304"
        ],
        [
         "6",
         "527"
        ],
        [
         "7",
         "1818"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    2141\n",
       "1     407\n",
       "2    3290\n",
       "3    1241\n",
       "4    4158\n",
       "5    2304\n",
       "6     527\n",
       "7    1818\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in VAL:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "fce893d9-4157-49a1-96d4-16c2ac1d971d",
       "rows": [
        [
         "0",
         "268"
        ],
        [
         "1",
         "51"
        ],
        [
         "2",
         "411"
        ],
        [
         "3",
         "155"
        ],
        [
         "4",
         "520"
        ],
        [
         "5",
         "288"
        ],
        [
         "6",
         "66"
        ],
        [
         "7",
         "227"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    268\n",
       "1     51\n",
       "2    411\n",
       "3    155\n",
       "4    520\n",
       "5    288\n",
       "6     66\n",
       "7    227\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in TEST:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7226a594-ab1e-44c2-acec-cb6ffc90ec29",
       "rows": [
        [
         "0",
         "267"
        ],
        [
         "1",
         "51"
        ],
        [
         "2",
         "411"
        ],
        [
         "3",
         "155"
        ],
        [
         "4",
         "520"
        ],
        [
         "5",
         "288"
        ],
        [
         "6",
         "66"
        ],
        [
         "7",
         "227"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "label_id\n",
       "0    267\n",
       "1     51\n",
       "2    411\n",
       "3    155\n",
       "4    520\n",
       "5    288\n",
       "6     66\n",
       "7    227\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = df_mapped[\"label_id\"].values\n",
    "\n",
    "train_idx, val_idx, test_idx = stratified_split_indices(\n",
    "    labels,\n",
    "    train_frac=cfg.train_frac,\n",
    "    val_frac=cfg.val_frac,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"✅ Stratified indices created.\")\n",
    "print(f\"Train size: {len(train_idx)}\")\n",
    "print(f\"Val size:   {len(val_idx)}\")\n",
    "print(f\"Test size:  {len(test_idx)}\")\n",
    "\n",
    "# Inspect label distribution across splits\n",
    "train_labels = df_mapped.iloc[train_idx][\"label_id\"]\n",
    "val_labels   = df_mapped.iloc[val_idx][\"label_id\"]\n",
    "test_labels  = df_mapped.iloc[test_idx][\"label_id\"]\n",
    "\n",
    "print(\"\\nLabel distribution in TRAIN:\")\n",
    "display(train_labels.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nLabel distribution in VAL:\")\n",
    "display(val_labels.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nLabel distribution in TEST:\")\n",
    "display(test_labels.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51819",
   "metadata": {},
   "source": [
    "### Step 6 - Encode Text with BERT Input Encoder\n",
    "\n",
    "BERT expects tokeized inputs:\n",
    "- `input_ids`: integers representing word pieces.\n",
    "- `attention_mask`: 1 for real tokens, 0 for padding.\n",
    "- `token_type_ids`: segment IDs (here mostly 0s since we use single sentences).\n",
    "\n",
    "We use:\n",
    "- `encode_splits` to:\n",
    "    - Build a tokenier based on your local BERT vocab.\n",
    "    - Turn text into tensors for each split (train/validation/test).\n",
    "\n",
    "We print:\n",
    "- Tensor shapes for each split.\n",
    "- Sequence length (should be equal `cfg.max_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e7ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text splits using BERT input encoder...\n",
      "\n",
      "Split: train\n",
      "  input_ids: shape=(15886, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(15886, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(15886, 256), dtype=torch.int64\n",
      "  labels: shape=(15886,), dtype=torch.int64\n",
      "\n",
      "Split: val\n",
      "  input_ids: shape=(1986, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(1986, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(1986, 256), dtype=torch.int64\n",
      "  labels: shape=(1986,), dtype=torch.int64\n",
      "\n",
      "Split: test\n",
      "  input_ids: shape=(1985, 256), dtype=torch.int64\n",
      "  token_type_ids: shape=(1985, 256), dtype=torch.int64\n",
      "  attention_mask: shape=(1985, 256), dtype=torch.int64\n",
      "  labels: shape=(1985,), dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding text splits using BERT input encoder...\")\n",
    "splits = encode_splits(df_mapped, train_idx, val_idx, test_idx, cfg)\n",
    "\n",
    "for split_name, tensors in splits.items():\n",
    "    print(f\"\\nSplit: {split_name}\")\n",
    "    for key, tensor in tensors.items():\n",
    "        print(f\"  {key}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d930bda",
   "metadata": {},
   "source": [
    "### Step 7 - Build PyTorch Datasets and DataLoaders\n",
    "\n",
    "PyTorch's `DataLoader`:\n",
    "- Handles batching (groups of exaqmples processed together).\n",
    "- Optionally shuffles data (for training).\n",
    "\n",
    "We:\n",
    "- Wrap each split (train, validation, test) into `TensorDictDataset`.\n",
    "- Build `DataLoaders`s with a batch size from `cfg.batch_size`.\n",
    "\n",
    "We print:\n",
    "- Number of batches for each split.\n",
    "- The shape of one batch from the trianing loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f1ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch DataLoaders...\n",
      "Train loader: 497 batch(es) with batch_size ~ 32\n",
      "Val loader: 32 batch(es) with batch_size ~ 64\n",
      "Test loader: 32 batch(es) with batch_size ~ 64\n",
      "\n",
      "Example TRAIN batch shapes:\n",
      "  input_ids:      (32, 256)\n",
      "  token_type_ids: (32, 256)\n",
      "  attention_mask: (32, 256)\n",
      "  labels:         (32,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Building PyTorch DataLoaders...\")\n",
    "loaders = build_dataloaders(splits, cfg)\n",
    "\n",
    "for name, loader in loaders.items():\n",
    "    num_batches = len(loader)\n",
    "    print(f\"{name.capitalize()} loader: {num_batches} batch(es) with batch_size ~ {loader.batch_size}\")\n",
    "\n",
    "# Peek at one training batch to understand the tensor shapes\n",
    "train_loader = loaders[\"train\"]\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "input_ids_batch, token_type_ids_batch, attention_mask_batch, labels_batch = first_batch\n",
    "\n",
    "print(\"\\nExample TRAIN batch shapes:\")\n",
    "print(f\"  input_ids:      {tuple(input_ids_batch.shape)}\")\n",
    "print(f\"  token_type_ids: {tuple(token_type_ids_batch.shape)}\")\n",
    "print(f\"  attention_mask: {tuple(attention_mask_batch.shape)}\")\n",
    "print(f\"  labels:         {tuple(labels_batch.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccc8e1",
   "metadata": {},
   "source": [
    "### Step 8 - Load BERT and Build the Classifier\n",
    "\n",
    "Now we:\n",
    "- Load the base BERT encoder from your local assets:\n",
    "    - Reads `config.json` and `pytorch_model.bin`.\n",
    "- Wrap it in a `BertTextClassifier`, which:\n",
    "    - Uses either `[CLS]` token or mean pooling (here: `cfg.pooling`).\n",
    "    - Adds a classifier head on top to predict your label classes. \n",
    "- Apply your fine-tuning policy (e.g., train only the last N transformer layers).\n",
    "\n",
    "We print:\n",
    "- Number of labels\n",
    "- model device (CPU or GPU).\n",
    "- Pooling strategy.\n",
    "- Fine-tuning Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08359a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT text classifier for 8 label(s)...\n",
      "\n",
      "✅ Model built.\n",
      "Pooling strategy: cls\n",
      "Model device: cuda:0\n",
      "Fine-tune policy: last_n, last_n=2, train_embeddings=False\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_to_id)\n",
    "print(f\"Building BERT text classifier for {num_labels} label(s)...\")\n",
    "\n",
    "model = build_bert_text_classifier_from_assets(cfg, num_labels=num_labels)\n",
    "\n",
    "print(\"\\n✅ Model built.\")\n",
    "print(f\"Pooling strategy: {model.pooling}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Fine-tune policy: {cfg.finetune_policy}, last_n={cfg.finetune_last_n}, train_embeddings={cfg.train_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3097bbe",
   "metadata": {},
   "source": [
    "### Step 9 - Train the Classifier\n",
    "\n",
    "**Time to train!**\n",
    "\n",
    "`train_text_classifer` will:\n",
    "- Build an optimizer (AdamW) using only trainable parameters.\n",
    "- Loop over epochs:\n",
    "    - Train on the training loader.\n",
    "    - Evaluate on the validation loader\n",
    "- Track:\n",
    "    - Training/validation loss.\n",
    "    - Training/validation accuracy.\n",
    "- Keep the best model state (based on validation accuracy). \n",
    "\n",
    "We print:\n",
    "- The per-epoch metrics (the function already does this).\n",
    "- The final training history in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661fb0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 01 | train_loss=0.8016 acc=0.738 | val_loss=0.2894 acc=0.908\n",
      "Epoch 02 | train_loss=0.2916 acc=0.912 | val_loss=0.1792 acc=0.948\n",
      "Epoch 03 | train_loss=0.1911 acc=0.945 | val_loss=0.1278 acc=0.963\n",
      "Epoch 04 | train_loss=0.1450 acc=0.957 | val_loss=0.1233 acc=0.967\n",
      "Epoch 05 | train_loss=0.1077 acc=0.967 | val_loss=0.1125 acc=0.968\n",
      "Epoch 06 | train_loss=0.0889 acc=0.974 | val_loss=0.1091 acc=0.971\n",
      "Epoch 07 | train_loss=0.0735 acc=0.978 | val_loss=0.1105 acc=0.973\n",
      "Epoch 08 | train_loss=0.0683 acc=0.981 | val_loss=0.1047 acc=0.972\n",
      "\n",
      "✅ Training complete.\n",
      "\n",
      "Training history:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "train_loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b1bd8a09-405b-43f8-8aa6-078dd2434d2e",
       "rows": [
        [
         "0",
         "1",
         "0.8015896266599033",
         "0.7375676696462293",
         "0.2893784795723638",
         "0.9078549848942599"
        ],
        [
         "1",
         "2",
         "0.2915528909274287",
         "0.9123756766964622",
         "0.17923280600062594",
         "0.9481369587109768"
        ],
        [
         "2",
         "3",
         "0.191137193055289",
         "0.9446053128540853",
         "0.12778841950736045",
         "0.9632426988922457"
        ],
        [
         "3",
         "4",
         "0.14501227698089883",
         "0.9573209114943976",
         "0.12332288848990265",
         "0.9667673716012085"
        ],
        [
         "4",
         "5",
         "0.107666590445313",
         "0.9669520332368123",
         "0.11254519451463976",
         "0.9682779456193353"
        ],
        [
         "5",
         "6",
         "0.08892382448603625",
         "0.974128163162533",
         "0.10909824714232011",
         "0.9707955689828801"
        ],
        [
         "6",
         "7",
         "0.0734825933548625",
         "0.9780939191741156",
         "0.11053401497299366",
         "0.973313192346425"
        ],
        [
         "7",
         "8",
         "0.0682531187729869",
         "0.9806748080070502",
         "0.10469062982542321",
         "0.972306143001007"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.801590</td>\n",
       "      <td>0.737568</td>\n",
       "      <td>0.289378</td>\n",
       "      <td>0.907855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.291553</td>\n",
       "      <td>0.912376</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>0.948137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.191137</td>\n",
       "      <td>0.944605</td>\n",
       "      <td>0.127788</td>\n",
       "      <td>0.963243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.145012</td>\n",
       "      <td>0.957321</td>\n",
       "      <td>0.123323</td>\n",
       "      <td>0.966767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.107667</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.112545</td>\n",
       "      <td>0.968278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.088924</td>\n",
       "      <td>0.974128</td>\n",
       "      <td>0.109098</td>\n",
       "      <td>0.970796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.073483</td>\n",
       "      <td>0.978094</td>\n",
       "      <td>0.110534</td>\n",
       "      <td>0.973313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.068253</td>\n",
       "      <td>0.980675</td>\n",
       "      <td>0.104691</td>\n",
       "      <td>0.972306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_acc  val_loss   val_acc\n",
       "0      1    0.801590   0.737568  0.289378  0.907855\n",
       "1      2    0.291553   0.912376  0.179233  0.948137\n",
       "2      3    0.191137   0.944605  0.127788  0.963243\n",
       "3      4    0.145012   0.957321  0.123323  0.966767\n",
       "4      5    0.107667   0.966952  0.112545  0.968278\n",
       "5      6    0.088924   0.974128  0.109098  0.970796\n",
       "6      7    0.073483   0.978094  0.110534  0.973313\n",
       "7      8    0.068253   0.980675  0.104691  0.972306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "history, best_state = train_text_classifier(model, loaders, cfg)\n",
    "\n",
    "print(\"\\n✅ Training complete.\")\n",
    "\n",
    "# Convert history to a DataFrame for easier viewing\n",
    "history_df = pd.DataFrame(history)\n",
    "print(\"\\nTraining history:\")\n",
    "display(history_df)\n",
    "\n",
    "# Note on Training Time:\n",
    "    # Training decoder and last 2 transformer blocks on BERT-large\n",
    "    # for 8 epochs using 1 NVIDIA RTX 2000 Ada Generation GPU \n",
    "    # costs ~ 103 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9031d4",
   "metadata": {},
   "source": [
    "### Step 10 - Save the Fine-tuned Model and Metadata \n",
    "\n",
    "After training, we want to:\n",
    "- Save:\n",
    "    - The full classifier (encoder + head).\n",
    "    - The fine-tuned encoder weights in BERT format.\n",
    "    - A small JSON metadata file with label mappings and training settings. \n",
    "\n",
    "The make it easy to:\n",
    "- Reload the model later\n",
    "- Run inference in an other script or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feac792c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned classifier and encoder weights...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '__dict__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving fine-tuned classifier and encoder weights...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msave_finetuned_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_to_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Fine-tuned model and metadata saved to:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(cfg.output_dir.resolve())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Python\\local-llm\\src\\local_llm\\training\\text_finetune.py:591\u001b[39m, in \u001b[36msave_finetuned_classifier\u001b[39m\u001b[34m(model, best_state, cfg, label_to_id, id_to_label)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# Append BERT config + head config + tokenizer setting into meta\u001b[39;00m\n\u001b[32m    590\u001b[39m bert_cfg_dict = model.bert.config.\u001b[34m__dict__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m head_cfg_dict = \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead_config\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\n\u001b[32m    593\u001b[39m meta = {\n\u001b[32m    594\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_labels\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(label_to_id),\n\u001b[32m    595\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel_to_id\u001b[39m\u001b[33m\"\u001b[39m: label_to_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    609\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhead_config\u001b[39m\u001b[33m\"\u001b[39m: head_cfg_dict,\n\u001b[32m    610\u001b[39m }\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33mfinetune_meta.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute '__dict__'"
     ]
    }
   ],
   "source": [
    "print(\"Saving fine-tuned classifier and encoder weights...\")\n",
    "save_finetuned_classifier(model, best_state, cfg, label_to_id, id_to_label)\n",
    "\n",
    "print(\"\\n✅ Fine-tuned model and metadata saved to:\")\n",
    "print(cfg.output_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d272f",
   "metadata": {},
   "source": [
    "### Step 11 - Evaluate on Test Set and Inspect Predictions\n",
    "\n",
    "Now we evaluate on the test split:\n",
    "- Compute:\n",
    "    - Test loss\n",
    "    - Test accuracy\n",
    "- Collect Predictions\n",
    "    - True label ids\n",
    "    - predicted label ids\n",
    "    - Confidence scores (max softmax probability)\n",
    "\n",
    "Then: \n",
    "- Merge predictions back with the originial test rows.\n",
    "- Save the combined table as a CSV (so you can inspect errors, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e1f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on TEST split...\n",
      "\n",
      "✅ Test evaluation complete.\n",
      "Test metrics:\n",
      "  loss: 0.1045\n",
      "  acc: 0.9753\n",
      "\n",
      "First 10 prediction rows (label_id, pred_label_id, pred_confidence):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pred_label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pred_confidence",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8e88ffb6-a3c8-48c4-94ab-3d2dd575cf93",
       "rows": [
        [
         "0",
         "4",
         "4",
         "0.9994331002235413"
        ],
        [
         "1",
         "0",
         "0",
         "0.9997469782829285"
        ],
        [
         "2",
         "1",
         "1",
         "0.9992184638977051"
        ],
        [
         "3",
         "3",
         "2",
         "0.8541951179504395"
        ],
        [
         "4",
         "4",
         "3",
         "0.9955018162727356"
        ],
        [
         "5",
         "4",
         "4",
         "0.9997372031211853"
        ],
        [
         "6",
         "4",
         "4",
         "0.9996836185455322"
        ],
        [
         "7",
         "2",
         "2",
         "0.9989176988601685"
        ],
        [
         "8",
         "3",
         "3",
         "0.9995642304420471"
        ],
        [
         "9",
         "4",
         "4",
         "0.9998261332511902"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_id</th>\n",
       "      <th>pred_label_id</th>\n",
       "      <th>pred_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.854195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.995502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_id  pred_label_id  pred_confidence\n",
       "0         4              4         0.999433\n",
       "1         0              0         0.999747\n",
       "2         1              1         0.999218\n",
       "3         3              2         0.854195\n",
       "4         4              3         0.995502\n",
       "5         4              4         0.999737\n",
       "6         4              4         0.999684\n",
       "7         2              2         0.998918\n",
       "8         3              3         0.999564\n",
       "9         4              4         0.999826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting test predictions merged with raw data to CSV...\n",
      "\n",
      "✅ Test predictions saved to CSV:\n",
      "C:\\Users\\Cameron.Webster\\Python\\local-llm\\artifacts\\finetune_large_bert\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on TEST split...\")\n",
    "test_metrics, test_preds = evaluate_on_split(model, splits[\"test\"], cfg)\n",
    "\n",
    "print(\"\\n✅ Test evaluation complete.\")\n",
    "print(\"Test metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nFirst 10 prediction rows (label_id, pred_label_id, pred_confidence):\")\n",
    "display(test_preds.head(10))\n",
    "\n",
    "# Align test raw rows with test_idx\n",
    "test_raw_df = df_mapped.iloc[test_idx]\n",
    "\n",
    "print(\"\\nExporting test predictions merged with raw data to CSV...\")\n",
    "csv_path = export_predictions_csv(test_preds, test_raw_df, id_to_label, cfg, split_name=\"test\")\n",
    "\n",
    "print(\"\\n✅ Test predictions saved to CSV:\")\n",
    "print(csv_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19202c8d",
   "metadata": {},
   "source": [
    "### Step 12 - Optional Quick Sanity Check on the Output CSV\n",
    "\n",
    "Finally, we can quickly reload the exported CSV and:\n",
    "- Check a few rows\n",
    "- look at the distribution of predicted labels\n",
    "\n",
    "This helps sanity-check that:\n",
    "- The file wrote correctly.\n",
    "- THe predictions look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6fd22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading exported predictions for sanity check...\n",
      "\n",
      "Loaded 1985 rows from predictions CSV.\n",
      "Columns in predictions CSV:\n",
      "['parsid', 'wbs', 'wbs_name', 'keyword', 'level_1', 'label_id', 'pred_label_id', 'pred_label', 'pred_confidence']\n",
      "\n",
      "First 5 rows from predictions CSV:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "parsid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wbs",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "wbs_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "level_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pred_label_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pred_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pred_confidence",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c8b01a85-a73a-4ca3-95f6-acaecf4d9551",
       "rows": [
        [
         "0",
         "1086",
         "1.07.02.09.45.80.20.31.25",
         "PSF Material Handling & Processing",
         "psf construction support facility handl material handl",
         "Process Equipment",
         "4",
         "4",
         "Process Equipment",
         "0.9994331002235411"
        ],
        [
         "1",
         "1155",
         "11.03.04.50649",
         "ARI-SXN Beamline Infrastructure",
         "structure",
         "Construction",
         "0",
         "0",
         "Construction",
         "0.9997469782829284"
        ],
        [
         "2",
         "1244",
         "3.32.08.42.02.05",
         "OSWDF 413 Critical Decision 4 (CD - 4) ESP",
         "disposal site prep facility construction",
         "D&D",
         "1",
         "1",
         "D&D",
         "0.9992184638977052"
        ],
        [
         "3",
         "1140",
         "1.03.04.01",
         "OPR, Specification, and Design Review Reports",
         "commissioning",
         "OPC Testing and Startup",
         "3",
         "2",
         "Design and NRE",
         "0.8541951179504395"
        ],
        [
         "4",
         "1092",
         "A.B.06.01.02.03",
         "Comm Support - Beamlines and Optical Systems",
         "optical commissioning",
         "Process Equipment",
         "4",
         "3",
         "OPC Testing and Startup",
         "0.9955018162727356"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsid</th>\n",
       "      <th>wbs</th>\n",
       "      <th>wbs_name</th>\n",
       "      <th>keyword</th>\n",
       "      <th>level_1</th>\n",
       "      <th>label_id</th>\n",
       "      <th>pred_label_id</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1086</td>\n",
       "      <td>1.07.02.09.45.80.20.31.25</td>\n",
       "      <td>PSF Material Handling &amp; Processing</td>\n",
       "      <td>psf construction support facility handl materi...</td>\n",
       "      <td>Process Equipment</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Process Equipment</td>\n",
       "      <td>0.999433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1155</td>\n",
       "      <td>11.03.04.50649</td>\n",
       "      <td>ARI-SXN Beamline Infrastructure</td>\n",
       "      <td>structure</td>\n",
       "      <td>Construction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Construction</td>\n",
       "      <td>0.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244</td>\n",
       "      <td>3.32.08.42.02.05</td>\n",
       "      <td>OSWDF 413 Critical Decision 4 (CD - 4) ESP</td>\n",
       "      <td>disposal site prep facility construction</td>\n",
       "      <td>D&amp;D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D&amp;D</td>\n",
       "      <td>0.999218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1140</td>\n",
       "      <td>1.03.04.01</td>\n",
       "      <td>OPR, Specification, and Design Review Reports</td>\n",
       "      <td>commissioning</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Design and NRE</td>\n",
       "      <td>0.854195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1092</td>\n",
       "      <td>A.B.06.01.02.03</td>\n",
       "      <td>Comm Support - Beamlines and Optical Systems</td>\n",
       "      <td>optical commissioning</td>\n",
       "      <td>Process Equipment</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>OPC Testing and Startup</td>\n",
       "      <td>0.995502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsid                        wbs  \\\n",
       "0    1086  1.07.02.09.45.80.20.31.25   \n",
       "1    1155             11.03.04.50649   \n",
       "2    1244           3.32.08.42.02.05   \n",
       "3    1140                 1.03.04.01   \n",
       "4    1092            A.B.06.01.02.03   \n",
       "\n",
       "                                        wbs_name  \\\n",
       "0             PSF Material Handling & Processing   \n",
       "1                ARI-SXN Beamline Infrastructure   \n",
       "2     OSWDF 413 Critical Decision 4 (CD - 4) ESP   \n",
       "3  OPR, Specification, and Design Review Reports   \n",
       "4   Comm Support - Beamlines and Optical Systems   \n",
       "\n",
       "                                             keyword                  level_1  \\\n",
       "0  psf construction support facility handl materi...        Process Equipment   \n",
       "1                                          structure             Construction   \n",
       "2           disposal site prep facility construction                      D&D   \n",
       "3                                      commissioning  OPC Testing and Startup   \n",
       "4                              optical commissioning        Process Equipment   \n",
       "\n",
       "   label_id  pred_label_id               pred_label  pred_confidence  \n",
       "0         4              4        Process Equipment         0.999433  \n",
       "1         0              0             Construction         0.999747  \n",
       "2         1              1                      D&D         0.999218  \n",
       "3         3              2           Design and NRE         0.854195  \n",
       "4         4              3  OPC Testing and Startup         0.995502  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted label counts:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "pred_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f10914d8-5412-4a65-9642-9336d8d31cff",
       "rows": [
        [
         "Process Equipment",
         "513"
        ],
        [
         "Design and NRE",
         "408"
        ],
        [
         "SEPM",
         "297"
        ],
        [
         "Construction",
         "263"
        ],
        [
         "Standard Equipment",
         "226"
        ],
        [
         "OPC Testing and Startup",
         "161"
        ],
        [
         "Site Preparation",
         "63"
        ],
        [
         "D&D",
         "54"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "pred_label\n",
       "Process Equipment          513\n",
       "Design and NRE             408\n",
       "SEPM                       297\n",
       "Construction               263\n",
       "Standard Equipment         226\n",
       "OPC Testing and Startup    161\n",
       "Site Preparation            63\n",
       "D&D                         54\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Reloading exported predictions for sanity check...\")\n",
    "preds_loaded = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(preds_loaded)} rows from predictions CSV.\")\n",
    "print(\"Columns in predictions CSV:\")\n",
    "print(preds_loaded.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows from predictions CSV:\")\n",
    "display(preds_loaded.head())\n",
    "\n",
    "print(\"\\nPredicted label counts:\")\n",
    "if \"pred_label\" in preds_loaded.columns:\n",
    "    display(preds_loaded[\"pred_label\"].value_counts())\n",
    "elif \"pred_label_id\" in preds_loaded.columns:\n",
    "    display(preds_loaded[\"pred_label_id\"].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
