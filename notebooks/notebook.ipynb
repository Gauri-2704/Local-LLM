{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd133ab0",
   "metadata": {},
   "source": [
    "# Run a simple training loop on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import local_llm as lllm\n",
    "from local_llm.pipelines.text_classification import (BertTextClassifier, ClassifierHeadConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1 SETUP\n",
    "assets_dir = lllm.setup_bert_base(\n",
    "    checkpoints=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/uncased_L-12_H-768_A-12\",\n",
    "    vocab=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/uncased_L-12_H-768_A-12/vocab.txt\",\n",
    "    config=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/uncased_L-12_H-768_A-12/bert_config.json\",\n",
    "    # optional; by default this would become ..\\assets\\bert-base-local\n",
    "    output_dir=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-base-local1\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Now assets_dir should contain:\n",
    "#   pytorch_model.bin\n",
    "#   config.json\n",
    "#   vocab.txt\n",
    "\n",
    "\n",
    "# # OPTION 2 SETUP\n",
    "# assets_dir = lllm.setup_bert_base(\n",
    "#     model_params=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-base-local/pytorch_model.bin\",\n",
    "#     vocab=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-base-local/vocab.txt\",\n",
    "#     config=r\"C:/Users/Cameron.Webster/Python/local-llm/assets/bert-base-local/config.json\",\n",
    "#     # output_dir optional; if omitted, uses the folder containing model_params\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build tokenizer\n",
    "encoder = lllm.build_bert_input_encoder(assets_dir, max_len=256, lowercase=True)\n",
    "\n",
    "texts = [\"This is a test.\", \"Another example.\"]\n",
    "encoded = [encoder.encode(t) for t in texts]\n",
    "\n",
    "input_ids = torch.tensor([e.input_ids for e in encoded])\n",
    "attention_mask = torch.tensor([e.attention_mask for e in encoded])\n",
    "token_type_ids = torch.tensor([e.token_type_ids for e in encoded])\n",
    "print(f\"input_ids:{input_ids}\")\n",
    "print(f\"attention_mask:{attention_mask}\")\n",
    "print(f\"token_type_ids:{token_type_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load BERT + classifier head\n",
    "num_labels = 8\n",
    "head_cfg = ClassifierHeadConfig(\n",
    "    hidden_sizes=(512, 256),\n",
    "    dropouts=(0.2, 0.2, 0.1),\n",
    "    use_layer_norm=True,\n",
    "    activation=\"relu\",\n",
    ")\n",
    "model = lllm.BertTextClassifier.from_pretrained(\n",
    "    assets_dir,\n",
    "    num_labels=num_labels, \n",
    "    pooling=\"cls\",\n",
    "    head_config=head_cfg,\n",
    ")\n",
    "\n",
    "model.set_finetune_policy(\"last_n\", last_n=2, train_embeddings=False)\n",
    "model.train()\n",
    "\n",
    "\n",
    "# # or inject a completely custome head: \n",
    "# custom_head = nn.Sequential(\n",
    "#     nn.Linear(768, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(256, 8),\n",
    "# )\n",
    "\n",
    "# model = BertTextClassifier.from_pretrained(\n",
    "#     assets_dir=\"assets/bert-base-local\",\n",
    "#     num_labels=8,\n",
    "#     head=custom_head,\n",
    "# )\n",
    "# model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO TRAIN ENCODER LAYERS WHILE STORING ANOTER VERSION\n",
    "from local_llm.training.config import TrainConfig\n",
    "from local_llm.training.head_trainer import train_classifier_head\n",
    "\n",
    "cfg = TrainConfig(\n",
    "    artifacts_root=Path(\"./artifacts\"),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    lr_head=2e-4,\n",
    "    lr_encoder=1e-5,\n",
    "    finetune_policy=\"last_n\",\n",
    "    finetune_last_n=2,\n",
    "    pooling=\"cls\",\n",
    ")\n",
    "\n",
    "best_head_state, best_encoder_state = train_classifier_head(\n",
    "    assets_dir=\"assets/bert-base-local\",\n",
    "    num_labels=8,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "# Save best head separately\n",
    "torch.save(best_head_state, cfg.artifacts_root / \"classifier_head.pt\")\n",
    "\n",
    "# Save finetuned encoder weights separately (never overwriting original assets)\n",
    "if best_encoder_state is not None:\n",
    "    torch.save(best_encoder_state, cfg.artifacts_root / \"encoder_finetuned.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Load base encoder for other tasks\n",
    "base_bert = BertTextClassifier.from_pretrained(\n",
    "    \"assets/bert-base-local\",\n",
    "    num_labels=8,\n",
    ")\n",
    "\n",
    "# Load finetuned encoder into a new classifier\n",
    "ft_model = BertTextClassifier.from_pretrained(\n",
    "    \"assets/bert-base-local\",\n",
    "    num_labels=8,\n",
    ")\n",
    "ft_model.bert.load_state_dict(torch.load(\"artifacts/encoder_finetuned.pt\"), strict=False)\n",
    "ft_model.classifier.load_state_dict(torch.load(\"artifacts/classifier_head.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70437576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Simple training step\n",
    "labels = torch.randint(0, num_labels, (input_ids.size(0),))\n",
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    labels=labels,\n",
    ")\n",
    "loss = out[\"loss\"]\n",
    "loss.backward()\n",
    "# optimizer.step(), optimizer.zero_grad(), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d292df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inference\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "    )[\"logits\"]\n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "print(preds)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
